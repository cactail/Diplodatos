{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "### Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones 2021\n",
    "Búsqueda y Recomendación para Textos Legales\n",
    "\n",
    "Mentor: Jorge E. Pérez Villella\n",
    "\n",
    "# Práctico Análisis y Visualización\n",
    "\n",
    "Integrantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos:\n",
    "\n",
    "Normalizar el corpus generado en el práctico anterior, teniendo en cuenta los siguientes aspectos:\n",
    "\n",
    "* tokenización, \n",
    "* pasar a minúsculas, \n",
    "* separar puntuación, \n",
    "* stemming y lematización, \n",
    "* eliminar stopwords (o no), \n",
    "* eliminar las palabras con frecuencia menor a n. \n",
    "\n",
    "Analizar las palabras más frecuentes de todo el corpus, por fuero y de 5 documentos. Compararlo con el resultado obtenido en el ejercicio anterior. Se observa algún cambio significativo?\n",
    "\n",
    "Hacer una explicación con ejemplos tomando algunas palabras al azar entre lo que es stemming y lemmatizing para entender que nos da cada uno de estos procesos y cual es conveniente utilizar en cada caso.\n",
    "\n",
    "Opcional:\n",
    "\n",
    "* Investigar que es Segmentación y compararlo con Tokenización. Ejemplificar con un documento.\n",
    "* Investigar NER (Named Entity Recognition - Reconocimiento de Entitades Nombradas). Buscar las Entidadas Nombradas mas frecuentes en todo el corpus y por fuero. \n",
    "\n",
    "\n",
    "Fecha de Entrega: 4 de julio de 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y -c conda-forge spacy=3.0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.0.6) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/Users/coviedo/opt/anaconda3/envs/diplodatos-ayvd/lib/python3.6/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY            VERSION                            \n",
      "es_core_news_md   >=3.0.0,<3.1.0   \u001b[38;5;2m3.0.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_sm    >=3.0.0,<3.1.0   \u001b[38;5;2m3.0.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import datetime\n",
    "from spacy.tokens.doc import Doc\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import lib.nlp_cba as nlp_cba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.6\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre con el cual se graba el data frame corpus de todas las sentencias. Cada sentencia es una fila\n",
    "corpus_file_name = \"corpus.csv\"\n",
    "\n",
    "#Nombre con el cual se graba el data frame que tiene el corpus sumarizado por fuero. Todo el texto de las sentencias de un\n",
    "# fuero esta en una sola fila\n",
    "agregated_corups_df_file_name = \"agregated_corpus.csv\"\n",
    "\n",
    "\n",
    "load_from_pickle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El data frame corpus_df se carga desde el archivo corpus.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"El data frame corpus_df se carga desde el archivo {corpus_file_name}\")\n",
    "corpus_df = pd.read_csv(corpus_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El data frame agregated_corups_df se carga desde el archivo agregated_corpus.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"El data frame agregated_corups_df se carga desde el archivo {agregated_corups_df_file_name}\")\n",
    "agregated_corups_df = pd.read_csv(agregated_corups_df_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datos de la causa sede  ciudad de córdoba.  de...</td>\n",
       "      <td>Documentos/MENORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unívoco 18900  fecha  04/04/2016  materia niñe...</td>\n",
       "      <td>Documentos/MENORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13/03/2013 juzgado de la niñez  juventud y vio...</td>\n",
       "      <td>Documentos/MENORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>los  autos  caratulados   a.   a.  -  denuncia...</td>\n",
       "      <td>Documentos/MENORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>juzg. de niñez  adolescencia y violencia famil...</td>\n",
       "      <td>Documentos/MENORES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          classifier\n",
       "0  datos de la causa sede  ciudad de córdoba.  de...  Documentos/MENORES\n",
       "1  unívoco 18900  fecha  04/04/2016  materia niñe...  Documentos/MENORES\n",
       "2  13/03/2013 juzgado de la niñez  juventud y vio...  Documentos/MENORES\n",
       "3  los  autos  caratulados   a.   a.  -  denuncia...  Documentos/MENORES\n",
       "4  juzg. de niñez  adolescencia y violencia famil...  Documentos/MENORES"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datos de la causa sede  ciudad de córdoba.  de...</td>\n",
       "      <td>Documentos/MENORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sala penal - tribunal superior  protocolo de s...</td>\n",
       "      <td>Documentos/PENAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto número  sesenta y seis  córdoba  cinco de...</td>\n",
       "      <td>Documentos/FAMILIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sala laboral - tribunal superior  protocolo de...</td>\n",
       "      <td>Documentos/LABORAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          classifier\n",
       "0  datos de la causa sede  ciudad de córdoba.  de...  Documentos/MENORES\n",
       "1  sala penal - tribunal superior  protocolo de s...    Documentos/PENAL\n",
       "2  auto número  sesenta y seis  córdoba  cinco de...  Documentos/FAMILIA\n",
       "3  sala laboral - tribunal superior  protocolo de...  Documentos/LABORAL"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agregated_corups_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nomalización de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el lenguaje español en spacy\n",
    "spacy_nlp = spacy.load(\"es_core_news_md\") \n",
    "spacy_nlp.max_length = 5000000\n",
    "\n",
    "Doc.set_extension('text_id', default=False, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos las STOP_WORDS que viene definidas por defecto\n",
    "#spacy.lang.es.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop_words que no estan contempladas en Spacy y consideramos necesario sacarlas\n",
    "\n",
    "customs_stop_words = ['y' , 'e' , 'a']\n",
    "\n",
    "for custom_stop_word in customs_stop_words:\n",
    "   spacy_nlp.vocab[custom_stop_word].is_stop = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clases para filtrar y transformar datos en Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyTextNormalizer:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.filters = []\n",
    "        self.transformers = []\n",
    "        self.documents = []\n",
    "        \n",
    "    def addFilter(self, filter):\n",
    "        self.filters.append(filter)\n",
    "        \n",
    "    def addTransformer(self, transformer):\n",
    "        self.transformers.append(transformer)\n",
    "        \n",
    "    \n",
    "    def fit(self , spacy_tuples):\n",
    "        self.documents = []\n",
    "        for doc,context in spacy_tuples:\n",
    "            doc._.text_id = context[\"text_id\"]\n",
    "            \n",
    "            self.documents.append(doc)\n",
    "        \n",
    "        \n",
    "    def normalize(self, filters = [], transformers = []):\n",
    "        documents = []\n",
    "        for doc in self.documents:\n",
    "            words = []\n",
    "            \n",
    "            for word in doc:\n",
    "                include = True\n",
    "\n",
    "                for filter in filters:\n",
    "                    include = filter.execute(word)\n",
    "                    if not include: \n",
    "                        break\n",
    "        \n",
    "                if include:\n",
    "                    \n",
    "                    transformed_res = []\n",
    "                    transformed_res.append(word)\n",
    "                    \n",
    "                    for transformer in transformers:\n",
    "                        transformed_res.append(transformer.transform(word))\n",
    "                        \n",
    "                    words.append(transformed_res)       \n",
    "        \n",
    "            documents.append(( doc ,words))\n",
    "        \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapMultipleCharsProcessor:\n",
    "\n",
    "    def process(self, text , replace_chars):\n",
    "        \n",
    "        for ch in replace_chars:\n",
    "            text = text.replace(ch[0],ch[1])\n",
    "\n",
    "        \n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola u este'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_replace = [('á' , 'a') , ('é' , 'e') , ('í', 'i') , ('ó' , 'o') , ('ú' , 'u')]\n",
    "mapMultipleCharsProcessor = MapMultipleCharsProcessor()\n",
    "mapMultipleCharsProcessor.process(\"Holá ú esté\" , chars_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RemoveStopWordsAndPuntctuationFilter:\n",
    "    \n",
    "    def execute(self, word):\n",
    "        return not word.is_stop and not word.is_punct\n",
    "\n",
    "\n",
    "class RemoveSpaceFilter:\n",
    "    def execute(self, word):\n",
    "        return not word.is_space\n",
    "    \n",
    "    \n",
    "class ToLowerCaseTransformer:\n",
    "    def transform(self, word):\n",
    "        return word.lower_\n",
    "    \n",
    "class ToLemaTransformer:\n",
    "    def transform(self, word):\n",
    "        return mapMultipleCharsProcessor.process(word.lemma_ , chars_replace)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtros y transformes que vamos a utilizar para normalizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtros y transformers\n",
    "\n",
    "removeSpaceFilter = RemoveSpaceFilter()\n",
    "removeStopWordsAndPuntctuationFilter = RemoveStopWordsAndPuntctuationFilter ()\n",
    "\n",
    "\n",
    "toLowerCaseTransformer = ToLowerCaseTransformer()\n",
    "toLemaTransformer = ToLemaTransformer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de filtros y transformers\n",
    "texto = [\n",
    "    (\"Pregunta?\", {\"text_id\": \"1\"} ),\n",
    "    (\"Pregunta.\", {\"text_id\": \"1\"} ),\n",
    "    (\"Estaba comiendo.\", {\"text_id\": \"1\"} ),\n",
    "    (\"Muchos espacios en     blanco .\", {\"text_id\": \"1\"} )\n",
    "    ]\n",
    "\n",
    "doc_tuples = spacy_nlp.pipe(texto , as_tuples=True , n_process=-1  )\n",
    "\n",
    "\n",
    "spacyTextNormalizer = SpacyTextNormalizer()\n",
    "\n",
    "spacyTextNormalizer.fit(doc_tuples)\n",
    "result = spacyTextNormalizer.normalize(transformers=[toLemaTransformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Pregunta?, [[Pregunta, 'Pregunta'], [?, '?']]),\n",
       " (Pregunta., [[Pregunta, 'Pregunta'], [., '.']]),\n",
       " (Estaba comiendo., [[Estaba, 'estar'], [comiendo, 'comer'], [., '.']]),\n",
       " (Muchos espacios en     blanco .,\n",
       "  [[Muchos, 'mucho'],\n",
       "   [espacios, 'espacio'],\n",
       "   [en, 'en'],\n",
       "   [    , '    '],\n",
       "   [blanco, 'blanco'],\n",
       "   [., '.']])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result[0][0]._.text_id\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extremadamente lento este enfoque para el caso de procesar el copus por fuero. El length del string del texto del corpus es 'grande' y \n",
    "# hace que sea lento el procesamiento\n",
    "# Código deprecado\n",
    "\n",
    "if False:\n",
    "    texto = [(agregated_corups_df.iloc[2,].text , {\"text_id\": \"1\"})]\n",
    "\n",
    "    doc_tuples = spacy_nlp.pipe(texto , as_tuples=True ,batch_size=50, n_process=4 , disable=[\"tok2vec\", \"tagger\",  \"attribute_ruler\"] )\n",
    "\n",
    "    result = spacyTextNormalizer.normalize(doc_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de procesamiento: 0:02:02.884425\n"
     ]
    }
   ],
   "source": [
    "#Armamos el array de tuplas a partir del data frame corpus_df. Usamos este data frame y no el agregated_corups_df puesto\n",
    "# que el array de texto por tupla es muy grande y Spacy requiere más memoria ademas de se notablemente lento\n",
    "\n",
    "if not load_from_pickle:\n",
    "\n",
    "    texto = corpus_df.apply( lambda x : (x['text'] , {\"text_id\": \"1\"}) , axis=1)\n",
    "    start_time = datetime.datetime.now()\n",
    "    doc_tuples = spacy_nlp.pipe(texto , as_tuples=True ,batch_size=50, n_process=4  )\n",
    "\n",
    "    spacyTextNormalizer.fit(doc_tuples)\n",
    "    stop_time = datetime.datetime.now()\n",
    "\n",
    "    print (f\"Tiempo de procesamiento: {stop_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poniendo a True serealizamos la instancia spacyTextNormalizer. Esto no permite reconstruir el objeto por medio del\n",
    "# archivo serealizado. Levantar el archivo y recrear el objeto es mucho más rápido de recostruir el objeto usando nlp.pipe y fit\n",
    "if True:\n",
    "    filehandler = open(\"normilizer.pkl\", 'wb') \n",
    "    pickle.dump(spacyTextNormalizer, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreamos el objeto spacyTextNormalizer desde un archivo, ver la explicación del punto anterior\n",
    "if load_from_pickle:\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    filehandler = open(\"normilizer.pkl\", 'rb') \n",
    "    spacyTextNormalizer = pickle.load(filehandler)\n",
    "    stop_time = datetime.datetime.now()  \n",
    "    \n",
    "    print (f\"Tiempo de procesamiento: {stop_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 243\n",
      "Tiempo de procesamiento: 0:00:04.007877\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "filters = [removeSpaceFilter,removeStopWordsAndPuntctuationFilter]\n",
    "\n",
    "transformers = [toLemaTransformer]\n",
    "\n",
    "result = spacyTextNormalizer.normalize(filters=filters, transformers=transformers)\n",
    "\n",
    "stop_time = datetime.datetime.now()\n",
    "\n",
    "print (f\"Cantidad de documentos: {len(result)}\")\n",
    "print (f\"Tiempo de procesamiento: {stop_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[242][1]\n",
    "\n",
    "\n",
    "def get_word_count(document ):\n",
    "    words = [ word[1] for word in document]\n",
    "    counter = Counter (words)\n",
    "    return pd.DataFrame(data = counter.most_common()  , columns= [ \"word\" , \"count\"])\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "result1 = get_word_count(result[1][1])\n",
    "result1['document_id'] = 1\n",
    "\n",
    "#result1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              word  count  id\n",
      "0               m.     29   1\n",
      "1              ley     24   1\n",
      "2        violencia     23   1\n",
      "3           medida     22   1\n",
      "4         familiar     20   1\n",
      "..             ...    ...  ..\n",
      "659      siguiente      1   1\n",
      "660      confirmar      1   1\n",
      "661  protocolicese      1   1\n",
      "662        hagasar      1   1\n",
      "663           dese      1   1\n",
      "\n",
      "[664 rows x 3 columns]\n",
      "              word  count  id\n",
      "0             niño     46   1\n",
      "1               fs     31   1\n",
      "2              art     29   1\n",
      "3           medida     28   1\n",
      "4          derecho     27   1\n",
      "..             ...    ...  ..\n",
      "917  protocolicese      1   1\n",
      "918        hagasar      1   1\n",
      "919           dese      1   1\n",
      "920            fdo      1   1\n",
      "921        wallace      1   1\n",
      "\n",
      "[922 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "value = 0\n",
    "\n",
    "for document in result:\n",
    "    res = get_word_count(document[1])\n",
    "    res['id'] = 1\n",
    "    print (res)\n",
    "    value = value +1 \n",
    "    if value == 2:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba Varias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos información de contexto del documento\n",
    "spacy.tokens.doc.Doc.set_extension('text_id' , default =False , force = True)\n",
    "doc_tuples = spacy_nlp.pipe([(\"Hola esto es una prueba que te parece\" , {\"text_id\" : \"text_id1_1\"})] , as_tuples=True , n_process=-1)\n",
    "docs = []\n",
    "\n",
    "for doc, context in doc_tuples:\n",
    "    doc._.text_id = context[\"text_id\"]\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        print(f\"ddd.{token.text} {token.is_sent_start}  {token.lemma_} {token.norm_} {token.pos_}{doc._.text_id} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:diplodatos-ayvd] *",
   "language": "python",
   "name": "conda-env-diplodatos-ayvd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
