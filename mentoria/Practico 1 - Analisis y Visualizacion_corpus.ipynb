{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "### Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones 2021\n",
    "Búsqueda y Recomendación para Textos Legales\n",
    "\n",
    "Mentor: Jorge E. Pérez Villella\n",
    "\n",
    "# Práctico Análisis y Visualización\n",
    "\n",
    "Integrantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos:\n",
    "\n",
    "* Generar un corpus con todos los documentos. \n",
    "\n",
    "* Dividir el corpus en tokens, graficar el histograma de frecuencia de palabras demostrando la ley Zipf. \n",
    "\n",
    "* Analizar palabras más frecuentes y menos frecuentes. Seleccionar 5 documentos de cada fuero y realizar el mismo análisis. ¿Se repiten las palabras? \n",
    "\n",
    "* Hacer lo mismo con n-gramas.\n",
    "\n",
    "* Visualizar la frecuencia de palabras en una nube de palabras.\n",
    "\n",
    "* Elaborar una breve conclusión de lo encontrado\n",
    "\n",
    "Fecha de Entrega: 6 de junio de 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generar un corpus con todos los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y -c anaconda spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/coviedo/opt/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/coviedo/opt/anaconda3/envs/diplodatos-ayvd\n",
      "\n",
      "  added / updated specs:\n",
      "    - dask\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    anaconda::ca-certificates-2020.10.14-0 --> conda-forge::ca-certificates-2020.12.5-h033912b_0\n",
      "  certifi                anaconda::certifi-2020.6.20-py36_0 --> conda-forge::certifi-2020.12.5-py36h79c6626_1\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.es.examples import sentences\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd \n",
    "import shutil\n",
    "\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "\n",
    "#client = Client(n_workers=4, threads_per_worker=2, memory_limit='1GB')\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-21f02d7979ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Documentos\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdirectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "root_path = \"Documentos\"\n",
    "\n",
    "directories = [x[0] for x in os.walk(root_path)]\n",
    "\n",
    "directories.pop(0)\n",
    "\n",
    "for directory in directories:\n",
    "    print (directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_corpus_df(directories):\n",
    "   corpus = []\n",
    "\n",
    "    for directory in directories:\n",
    "      \n",
    "        file_list = glob.glob(os.path.join(os.getcwd(), directory, \"*.txt\"))\n",
    "\n",
    "\n",
    "        for file_path in file_list:\n",
    "            with open(file_path) as f_input:\n",
    "                corpus.append([f_input.read() , directory])\n",
    "\n",
    "    return dd.from_pandas(data = pd.DataFrame(corpus, columns=[\"text\", \"classifier\"] ), npartitions=4 )\n",
    "    #return pd.DataFrame(corpus, columns=[\"text\", \"classifier\"] )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = generate_corpus_df(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classifier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 4 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 text classifier\n",
       "npartitions=4                   \n",
       "0              object     object\n",
       "63                ...        ...\n",
       "126               ...        ...\n",
       "189               ...        ...\n",
       "250               ...        ...\n",
       "Dask Name: from_pandas, 4 tasks"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_file(directory, corpus_file_name): \n",
    "    \n",
    "    outfilename = os.path.join(os.getcwd(), directory , corpus_file_name )\n",
    "    input_files = os.path.join(os.getcwd(), directory , '*.txt' )\n",
    "    \n",
    "    with open(outfilename, 'wb') as outfile:\n",
    "        for filename in glob.glob(input_files):\n",
    "            if filename == outfilename:\n",
    "                # don't want to copy the output into the output\n",
    "                continue\n",
    "            with open(filename, 'rb') as readfile:\n",
    "                shutil.copyfileobj(readfile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos el corpus de cada fuero en el archivo corpust.txt. Hacemos esto puesto que es muchos menos \n",
    "# costoso que pasar a un unico string los valores del data frame\n",
    "\n",
    "corpus_file_name = \"corpus.txt\"\n",
    "for directory in directories:\n",
    "    generate_corpus_file(directory, corpus_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_agregate_df_corpus(directories, corpus_file_name): \n",
    "    \n",
    "    agregate_df_corpus = pd.DataFrame()\n",
    "        \n",
    "    for directory in directories: \n",
    "        \n",
    "        corpus_file = os.path.join(os.getcwd(), directory , corpus_file_name ) \n",
    "        print(corpus_file)\n",
    "        with open(corpus_file, 'r') as file:\n",
    "            data = file.read().replace('\\n', ' ')\n",
    "            print(data[:10])\n",
    "            agregate_df_corpus.append({'text' : data, 'tag' : directory } , ignore_index = True)\n",
    "            print (\"Se agrego\")\n",
    "    \n",
    "    return agregate_df_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_agregate_df_corpus_2(directories, corpus_file_name): \n",
    "    \n",
    "    agregate_df_corpus = pd.DataFrame(columns=['text', 'tag'])\n",
    "        \n",
    "    for directory in directories: \n",
    "        \n",
    "        corpus_file = os.path.join(os.getcwd(), directory , corpus_file_name ) \n",
    "        data = open(corpus_file, 'r').read()\n",
    "        agregate_df_corpus = agregate_df_corpus.append({'text' : data, 'tag' : directory } , ignore_index = True)\n",
    "        \n",
    "    return agregate_df_corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agregate_df_corpus = generate_agregate_df_corpus_2(directories ,corpus_file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DATOS DE LA CAUSA\\nSede: Ciudad de Córdoba. \\n...</td>\n",
       "      <td>Documentos/MENORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SALA PENAL - TRIBUNAL SUPERIOR\\n\\nProtocolo de...</td>\n",
       "      <td>Documentos/PENAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTO NÚMERO: sesenta y seis \\nCórdoba, cinco d...</td>\n",
       "      <td>Documentos/FAMILIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>Documentos/LABORAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                 tag\n",
       "0  DATOS DE LA CAUSA\\nSede: Ciudad de Córdoba. \\n...  Documentos/MENORES\n",
       "1  SALA PENAL - TRIBUNAL SUPERIOR\\n\\nProtocolo de...    Documentos/PENAL\n",
       "2  AUTO NÚMERO: sesenta y seis \\nCórdoba, cinco d...  Documentos/FAMILIA\n",
       "3  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...  Documentos/LABORAL"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agregate_df_corpus[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir el corpus en tokens, graficar el histograma de frecuencia de palabras demostrando la ley Zipf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no funciona, consume mucha memoria\n",
    "def generate_corpus_by_tag(df, tag):\n",
    "    \n",
    "    corpus = corpus_df[corpus_df['classifier'] == classifier ].text.to_list()\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no funciona, consume mucha memoria\n",
    "spacy_nlp.max_length = 3000000\n",
    "for directory in directories:\n",
    "    print (f\"Generando corpus de {directory}\")\n",
    "    corpus = \"\".join(corpus_df[corpus_df['classifier'] == classifier ].text.to_list())\n",
    "    corpus_doc = spacy_nlp(corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp.max_length = 7000000\n",
    "doc = spacy_nlp(agregate_df_corpus[agregate_df_corpus['tag'] == 'Documentos/MENORES'].text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text\n",
    "         for token in doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = word_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 11660),\n",
       " ('y', 10122),\n",
       " ('a', 8780),\n",
       " ('\\n\\n', 2170),\n",
       " ('fs', 2010),\n",
       " ('niño', 1666),\n",
       " ('\\n', 1410),\n",
       " ('o', 1158),\n",
       " ('Sra.', 1156),\n",
       " ('art', 1106)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_penal = spacy_nlp(agregate_df_corpus[agregate_df_corpus['tag'] == 'Documentos/PENAL'].text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text\n",
    "         for token in doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = word_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:diplodatos-ayvd] *",
   "language": "python",
   "name": "conda-env-diplodatos-ayvd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
